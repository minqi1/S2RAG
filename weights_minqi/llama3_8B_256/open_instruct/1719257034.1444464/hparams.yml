checkpointing_steps: '600'
config_name: null
dataset_config_name: null
dataset_name: null
gradient_accumulation_steps: 16
learning_rate: 1.5e-05
logging_steps: 1
lora_alpha: 16
lora_dropout: 0.05
lora_rank: 256
low_cpu_mem_usage: false
lr_scheduler_type: linear
max_seq_length: 1024
max_train_steps: 1595
model_name_or_path: /apdcephfs_qy3/share_4983883/ping_test/ping/hf_model/Meta-Llama-3-8B-Instruct
num_train_epochs: 5
output_dir: ../weights_minqi/generator_weights/llama3_8B_allft_left_256_3/
overwrite_cache: false
per_device_train_batch_size: 1
preprocessing_num_workers: 16
report_to: tensorboard
resume_from_checkpoint: null
seed: null
tokenizer_name: /apdcephfs_qy3/share_4983883/ping_test/ping/hf_model/Meta-Llama-3-8B-Instruct
train_file: /apdcephfs_qy3/share_4983883/ping_test/rag/minqi_dev/self-rag-main/data_training/train_data_fusion_w_exp.jsonl
use_flash_attn: false
use_left_padding: true
use_lora: true
use_slow_tokenizer: true
use_special_tokens: false
warmup_ratio: 0.03
weight_decay: 0.0
with_tracking: true
